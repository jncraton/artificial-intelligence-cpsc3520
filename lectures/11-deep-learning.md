# Deep Learning

## Early Struggles (1990s)

- Neural Networks underperforming
- Limited computing power
- Lack of large datasets

## Computing Power in the 1990s

- Personal computers becoming more common
- Early GPUs for graphics
- Moore's Law: doubling of transistors every two years

## LeNet

- [Yann LeCun's convolutional neural network](https://en.wikipedia.org/wiki/LeNet)
- Handwritten digit recognition
- Paved the way for future CNNs

---

![LeNet Architecture](https://upload.wikimedia.org/wikipedia/commons/3/35/LeNet-5_architecture.svg){height=540px}

## The Turn of the Millennium

- Internet explosion
- More data available
- Improved algorithms

## AlexNet (2012)

- Won ImageNet competition
- 8 layers, 60 million parameters

---

![AlexNet Architecture](https://upload.wikimedia.org/wikipedia/commons/a/a9/AlexNet_Original_block_diagram.svg)

## Impact of AlexNet

- Revived interest in deep learning
- Boosted GPU usage
- Opened doors for more complex models

## ResNet (2015)

- Residual connections
- Solved vanishing gradient problem

---

![ResNet Block](https://upload.wikimedia.org/wikipedia/commons/thumb/b/ba/ResBlock.png/1024px-ResBlock.png)

## Transformers (2017)

- Attention mechanism
- Parallelizable
- State-of-the-art in NLP

---

![Transformer Block](https://upload.wikimedia.org/wikipedia/commons/thumb/3/34/Transformer%2C_full_architecture.png/456px-Transformer%2C_full_architecture.png)

## Societal Impacts

- AI in healthcare, finance, and more
- Ethical concerns
- Job displacement

## Computing Power Today

- Cloud computing
- Specialized hardware (TPUs, etc.)
- Training models on massive datasets

## Quotes

Just as electricity transformed almost everything 100 years ago, today I actually have a hard time thinking of an industry that I don't think AI will transform in the next several years.

[Andrew Ng](https://www.gsb.stanford.edu/insights/andrew-ng-why-ai-new-electricity)
