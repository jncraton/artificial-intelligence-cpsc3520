# Deep Learning

## Early Struggles (1990s)

- Neural Networks underperforming
- Limited computing power
- Lack of large datasets

## Computing Power in the 1990s

- Personal computers becoming more common
- Early GPUs for graphics
- Moore's Law: doubling of transistors every two years

## LeNet

- [Yann LeCun's convolutional neural network](https://en.wikipedia.org/wiki/LeNet)
- Handwritten digit recognition
- Paved the way for future CNNs

---

![LeNet Architecture](https://upload.wikimedia.org/wikipedia/commons/3/35/LeNet-5_architecture.svg){height=540px}

## The Turn of the Millennium

- Internet explosion
- More data available
- Improved algorithms

## AlexNet (2012)

- Won ImageNet competition
- 8 layers, 60 million parameters
- [Paper](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)

---

![AlexNet Architecture](https://upload.wikimedia.org/wikipedia/commons/a/a9/AlexNet_Original_block_diagram.svg)

---

"We trained the network for roughly 90 cycles through the training set of 1.2 million images, which took five to six days on two NVIDIA GTX 580 3GB GPUs"

---

"Our network achieves top-1 and top-5 test set error rates of 37.5% and 17.0%. The best performance achieved during the ILSVRC-2010 competition was 47.1% and 28.2%"

## Impact of AlexNet

- Revived interest in deep learning
- Boosted GPU usage
- Opened doors for more complex models

## ResNet (2015)

- Residual connections
- Solved vanishing gradient problem

---

![ResNet Block](https://upload.wikimedia.org/wikipedia/commons/thumb/b/ba/ResBlock.png/1024px-ResBlock.png)

## Transformers (2017)

- Attention mechanism
- Parallelizable
- State-of-the-art in NLP

---

![Transformer Block](https://upload.wikimedia.org/wikipedia/commons/thumb/3/34/Transformer%2C_full_architecture.png/456px-Transformer%2C_full_architecture.png)

## Computing Power Today

- Cloud computing
- Specialized hardware (TPUs, etc.)
- Training models on massive datasets

## Societal Impacts

## Quotes

Just as electricity transformed almost everything 100 years ago, today I actually have a hard time thinking of an industry that I don't think AI will transform in the next several years.

[Andrew Ng](https://www.gsb.stanford.edu/insights/andrew-ng-why-ai-new-electricity)
