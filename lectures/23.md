Transformers
============

---

![Transformer Architecture ([Attention is All you Need](https://arxiv.org/pdf/1706.03762.pdf))](media/transformer.png){height=540px}

---

![GPT-1 Decoder-only Transformer](media/gpt1.png){height=540px}

---

![Recurrent Neural Network](https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg)
---

Attention
---------

---

![Scaled Dot-product Attention](media/scaled-dot-product-attention.png)

---

![Multihead Attention](media/multihead-attention.png)

Self-attention Advantages
-------------------------

- Reduced computational complexity per layer
- Increased paralellism
- Reduced path length for long-range dependencies